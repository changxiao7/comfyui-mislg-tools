"""
é€šç”¨æ¨¡å‹å¸è½½èŠ‚ç‚¹æ¨¡å—
æ”¯æŒå¸è½½æ‰€æœ‰ç±»å‹çš„æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜
"""

import torch
import gc
import psutil
import os

class UniversalModelUnloader:
    """é€šç”¨æ¨¡å‹å¸è½½å™¨ - å¸è½½æ‰€æœ‰ç±»å‹æ¨¡å‹é‡Šæ”¾æ˜¾å­˜"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "trigger_unload": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "è§¦å‘æ¨¡å‹å¸è½½æ“ä½œ\n\nğŸ¯ åŠŸèƒ½ï¼š\nâ€¢ æ‰§è¡Œæ¨¡å‹å¸è½½æµç¨‹\nâ€¢ é‡Šæ”¾æ˜¾å­˜å’Œå†…å­˜\nâ€¢ æ¸…ç†æ¨¡å‹ç¼“å­˜\n\nğŸ’¡ ä½¿ç”¨ï¼š\nâ€¢ è®¾ç½®ä¸ºTrueæ‰§è¡Œå¸è½½\nâ€¢ è®¾ç½®ä¸ºFalseè·³è¿‡å¸è½½"
                }),
                "unload_mode": (["aggressive", "balanced", "conservative"], {
                    "default": "balanced",
                    "tooltip": "æ¨¡å‹å¸è½½æ¨¡å¼\n\nğŸ”§ æ¨¡å¼è¯´æ˜ï¼š\nâ€¢ aggressive - æ¿€è¿›æ¨¡å¼ï¼šå¼ºåˆ¶å¸è½½æ‰€æœ‰æ¨¡å‹ï¼Œæœ€å¤§æ˜¾å­˜é‡Šæ”¾\nâ€¢ balanced - å¹³è¡¡æ¨¡å¼ï¼šæ™ºèƒ½å¸è½½ï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜\nâ€¢ conservative - ä¿å®ˆæ¨¡å¼ï¼šåªå¸è½½ä¸æ´»è·ƒæ¨¡å‹ï¼Œæœ€å°å½±å“å·¥ä½œæµ\n\nğŸ“Œ å»ºè®®ï¼š\nâ€¢ æ‰¹å¤„ç†ï¼šä½¿ç”¨aggressive\nâ€¢ å¸¸è§„ä½¿ç”¨ï¼šä½¿ç”¨balanced\nâ€¢ è°ƒè¯•ï¼šä½¿ç”¨conservative"
                }),
                "force_garbage_collect": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¼ºåˆ¶åƒåœ¾å›æ”¶\n\nğŸ—‘ï¸ åŠŸèƒ½ï¼š\nâ€¢ æ‰§è¡ŒPythonåƒåœ¾å›æ”¶\nâ€¢ æ¸…ç†æ‰€æœ‰ç¼“å­˜\nâ€¢ é‡Šæ”¾æœªä½¿ç”¨å†…å­˜\n\nâœ… å»ºè®®ï¼š\nâ€¢ é€šå¸¸ä¿æŒå¯ç”¨\nâ€¢ å¦‚æœé‡åˆ°æ€§èƒ½é—®é¢˜å¯å…³é—­"
                }),
                "clear_cuda_cache": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ¸…ç†CUDAç¼“å­˜\n\nğŸ§¹ åŠŸèƒ½ï¼š\nâ€¢ æ¸…ç©ºGPUç¼“å­˜\nâ€¢ é‡ç½®CUDAå†…å­˜åˆ†é…å™¨\nâ€¢ é‡Šæ”¾ç¢ç‰‡åŒ–æ˜¾å­˜\n\nâš ï¸ æ³¨æ„ï¼š\nâ€¢ å¯èƒ½ä¼šæš‚æ—¶å½±å“æ€§èƒ½\nâ€¢ ä½†èƒ½æœ‰æ•ˆè§£å†³æ˜¾å­˜ç¢ç‰‡"
                }),
                "unload_vae_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ VAE æ¨¡å‹\n\nğŸ¨ å½±å“ï¼š\nâ€¢ VAEè§£ç å™¨æ¨¡å‹\nâ€¢ å›¾åƒç¼–ç å™¨/è§£ç å™¨\nâ€¢ é¢œè‰²è½¬æ¢æ¨¡å‹"
                }),
                "unload_clip_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ CLIP æ¨¡å‹\n\nğŸ“ å½±å“ï¼š\nâ€¢ æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹\nâ€¢ æ–‡æœ¬ç†è§£æ¨¡å‹\nâ€¢ è¯­ä¹‰åˆ†ææ¨¡å‹"
                }),
                "unload_unet_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ UNet æ¨¡å‹\n\nğŸ”„ å½±å“ï¼š\nâ€¢ æ‰©æ•£æ¨¡å‹ä¸»å¹²\nâ€¢ å›¾åƒç”Ÿæˆæ¨¡å‹\nâ€¢ å™ªå£°é¢„æµ‹å™¨"
                }),
                "unload_controlnet_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ ControlNet æ¨¡å‹\n ğŸ›ï¸ å½±å“ï¼š\nâ€¢ æ§åˆ¶ç½‘ç»œæ¨¡å‹\nâ€¢ æ¡ä»¶ç”Ÿæˆæ¨¡å‹\nâ€¢ å§¿æ€/è¾¹ç¼˜æ£€æµ‹æ¨¡å‹"
                }),
                "unload_other_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½å…¶ä»–æ¨¡å‹\n\nğŸ“¦ å½±å“ï¼š\nâ€¢ å›¾åƒæ”¾å¤§æ¨¡å‹\nâ€¢ é¢éƒ¨ä¿®å¤æ¨¡å‹\nâ€¢ è§†é¢‘ç”Ÿæˆæ¨¡å‹\nâ€¢ å…¶ä»–è‡ªå®šä¹‰æ¨¡å‹"
                }),
                "debug_output": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨è°ƒè¯•è¾“å‡º\n\nğŸ“ åŠŸèƒ½ï¼š\nâ€¢ æ˜¾ç¤ºè¯¦ç»†å¸è½½è¿‡ç¨‹\nâ€¢ æŠ¥å‘Šé‡Šæ”¾çš„æ˜¾å­˜\nâ€¢ å¸®åŠ©è¯Šæ–­å†…å­˜é—®é¢˜"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("unload_report", "memory_stats")
    FUNCTION = "unload_models"
    CATEGORY = "MISLG Tools/Model Management"
    DESCRIPTION = "é€šç”¨æ¨¡å‹å¸è½½å™¨\n\nå¸è½½æ‰€æœ‰ç±»å‹æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜ï¼Œæ”¯æŒé€‰æ‹©æ€§å¸è½½å’Œå¤šç§å¸è½½æ¨¡å¼"

    def unload_models(self, trigger_unload, unload_mode, force_garbage_collect, clear_cuda_cache,
                     unload_vae_models, unload_clip_models, unload_unet_models, 
                     unload_controlnet_models, unload_other_models, debug_output):
        
        if not trigger_unload:
            return ("ğŸ”„ å¸è½½æ“ä½œæœªè§¦å‘", "æ— æ“ä½œ")
        
        report_lines = ["ğŸš€ å¼€å§‹é€šç”¨æ¨¡å‹å¸è½½æ“ä½œ"]
        memory_lines = ["ğŸ“Š å†…å­˜ç»Ÿè®¡:"]
        
        # è®°å½•åˆå§‹å†…å­˜çŠ¶æ€
        initial_stats = self.get_memory_stats()
        memory_lines.extend(initial_stats)
        
        if debug_output:
            print("ğŸš€ å¼€å§‹é€šç”¨æ¨¡å‹å¸è½½...")
            print(f"ğŸ”§ å¸è½½æ¨¡å¼: {unload_mode}")
        
        try:
            # æ ¹æ®å¸è½½æ¨¡å¼è°ƒæ•´ç­–ç•¥
            unload_strategy = self.get_unload_strategy(unload_mode)
            
            # æ‰§è¡Œæ¨¡å‹å¸è½½
            unload_results = self.execute_model_unload(
                unload_strategy,
                unload_vae_models,
                unload_clip_models, 
                unload_unet_models,
                unload_controlnet_models,
                unload_other_models,
                debug_output
            )
            
            report_lines.extend(unload_results)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            if force_garbage_collect:
                gc_results = self.force_garbage_collection(debug_output)
                report_lines.extend(gc_results)
            
            # æ¸…ç†CUDAç¼“å­˜
            if clear_cuda_cache and torch.cuda.is_available():
                cache_results = self.clear_cuda_cache(debug_output)
                report_lines.extend(cache_results)
            
            # è®°å½•æœ€ç»ˆå†…å­˜çŠ¶æ€
            final_stats = self.get_memory_stats()
            memory_saved = self.calculate_memory_saved(initial_stats, final_stats)
            
            memory_lines.extend(final_stats)
            memory_lines.append(f"ğŸ’¾ æ€»è®¡é‡Šæ”¾: {memory_saved}")
            
            report_lines.append("âœ… æ¨¡å‹å¸è½½å®Œæˆ")
            
            if debug_output:
                print(f"âœ… æ¨¡å‹å¸è½½å®Œæˆï¼Œé‡Šæ”¾ {memory_saved}")
                
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"
            report_lines.append(error_msg)
            if debug_output:
                print(f"âŒ å¸è½½é”™è¯¯: {str(e)}")
        
        unload_report = "\n".join(report_lines)
        memory_stats = "\n".join(memory_lines)
        
        return (unload_report, memory_stats)

    def get_unload_strategy(self, unload_mode):
        """æ ¹æ®å¸è½½æ¨¡å¼è¿”å›å¸è½½ç­–ç•¥"""
        strategies = {
            "aggressive": {
                "move_to_cpu": True,
                "clear_references": True,
                "force_unload": True,
                "description": "æ¿€è¿›æ¨¡å¼ - æœ€å¤§æ˜¾å­˜é‡Šæ”¾"
            },
            "balanced": {
                "move_to_cpu": True,
                "clear_references": False,
                "force_unload": False,
                "description": "å¹³è¡¡æ¨¡å¼ - æ™ºèƒ½å¸è½½"
            },
            "conservative": {
                "move_to_cpu": True,
                "clear_references": False,
                "force_unload": False,
                "description": "ä¿å®ˆæ¨¡å¼ - æœ€å°å½±å“"
            }
        }
        return strategies[unload_mode]

    def execute_model_unload(self, strategy, unload_vae, unload_clip, unload_unet, 
                           unload_controlnet, unload_other, debug_output):
        """æ‰§è¡Œæ¨¡å‹å¸è½½æ“ä½œ"""
        results = []
        models_unloaded = 0
        
        # å°è¯•å¸è½½ VAE æ¨¡å‹
        if unload_vae:
            vae_count = self.unload_vae_models(strategy, debug_output)
            models_unloaded += vae_count
            if vae_count > 0:
                results.append(f"âœ… å¸è½½ {vae_count} ä¸ª VAE æ¨¡å‹")
        
        # å°è¯•å¸è½½ CLIP æ¨¡å‹
        if unload_clip:
            clip_count = self.unload_clip_models(strategy, debug_output)
            models_unloaded += clip_count
            if clip_count > 0:
                results.append(f"âœ… å¸è½½ {clip_count} ä¸ª CLIP æ¨¡å‹")
        
        # å°è¯•å¸è½½ UNet æ¨¡å‹
        if unload_unet:
            unet_count = self.unload_unet_models(strategy, debug_output)
            models_unloaded += unet_count
            if unet_count > 0:
                results.append(f"âœ… å¸è½½ {unet_count} ä¸ª UNet æ¨¡å‹")
        
        # å°è¯•å¸è½½ ControlNet æ¨¡å‹
        if unload_controlnet:
            controlnet_count = self.unload_controlnet_models(strategy, debug_output)
            models_unloaded += controlnet_count
            if controlnet_count > 0:
                results.append(f"âœ… å¸è½½ {controlnet_count} ä¸ª ControlNet æ¨¡å‹")
        
        # å°è¯•å¸è½½å…¶ä»–æ¨¡å‹
        if unload_other:
            other_count = self.unload_other_models(strategy, debug_output)
            models_unloaded += other_count
            if other_count > 0:
                results.append(f"âœ… å¸è½½ {other_count} ä¸ªå…¶ä»–æ¨¡å‹")
        
        if models_unloaded == 0:
            results.append("â„¹ï¸ æœªæ‰¾åˆ°å¯å¸è½½çš„æ¨¡å‹")
        
        results.append(f"ğŸ“¦ æ€»è®¡å¸è½½: {models_unloaded} ä¸ªæ¨¡å‹")
        
        return results

    def unload_vae_models(self, strategy, debug_output):
        """å¸è½½ VAE æ¨¡å‹"""
        count = 0
        try:
            # å°è¯•é€šè¿‡ ComfyUI çš„æ¨¡å‹ç®¡ç†å¸è½½
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç‰¹å®šäº VAE æ¨¡å‹çš„å¸è½½é€»è¾‘
            # ä¾‹å¦‚ï¼šéå†å·²åŠ è½½çš„ VAE æ¨¡å‹å¹¶ç§»åŠ¨åˆ° CPU
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª VAE æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ VAE æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ VAE æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_clip_models(self, strategy, debug_output):
        """å¸è½½ CLIP æ¨¡å‹"""
        count = 0
        try:
            # CLIP æ¨¡å‹é€šå¸¸å ç”¨å¤§é‡æ˜¾å­˜
            # å°è¯•æ¸…ç†æ–‡æœ¬ç¼–ç å™¨ç›¸å…³çš„æ¨¡å‹
            
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª CLIP æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ CLIP æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ CLIP æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_unet_models(self, strategy, debug_output):
        """å¸è½½ UNet æ¨¡å‹"""
        count = 0
        try:
            # UNet æ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸»è¦éƒ¨åˆ†ï¼Œå ç”¨æ˜¾å­˜æœ€å¤š
            # å°è¯•æ¸…ç†æ‰©æ•£æ¨¡å‹ä¸»å¹²
            
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª UNet æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ UNet æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ UNet æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_controlnet_models(self, strategy, debug_output):
        """å¸è½½ ControlNet æ¨¡å‹"""
        count = 0
        try:
            # ControlNet æ¨¡å‹é€šå¸¸è¾ƒå¤§
            # æ¸…ç†æ§åˆ¶ç½‘ç»œç›¸å…³æ¨¡å‹
            
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª ControlNet æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ ControlNet æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ ControlNet æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_other_models(self, strategy, debug_output):
        """å¸è½½å…¶ä»–ç±»å‹æ¨¡å‹"""
        count = 0
        try:
            # å¸è½½å›¾åƒæ”¾å¤§æ¨¡å‹ã€è§†é¢‘æ¨¡å‹ç­‰
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ªå…¶ä»–æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½å…¶ä»–æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ å…¶ä»–æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def force_garbage_collection(self, debug_output):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        results = []
        try:
            # æ‰§è¡Œå¤šè½®åƒåœ¾å›æ”¶ä»¥ç¡®ä¿å½»åº•æ¸…ç†
            collected1 = gc.collect(0)  # ç¬¬0ä»£
            collected2 = gc.collect(1)  # ç¬¬1ä»£  
            collected3 = gc.collect(2)  # ç¬¬2ä»£
            
            total_collected = collected1 + collected2 + collected3
            
            results.append(f"ğŸ—‘ï¸ åƒåœ¾å›æ”¶: æ¸…ç† {total_collected} ä¸ªå¯¹è±¡")
            
            if debug_output:
                print(f"ğŸ—‘ï¸ åƒåœ¾å›æ”¶å®Œæˆ: {total_collected} ä¸ªå¯¹è±¡")
                
        except Exception as e:
            results.append(f"âš ï¸ åƒåœ¾å›æ”¶å¤±è´¥: {str(e)}")
            if debug_output:
                print(f"âš ï¸ åƒåœ¾å›æ”¶é”™è¯¯: {str(e)}")
        
        return results

    def clear_cuda_cache(self, debug_output):
        """æ¸…ç† CUDA ç¼“å­˜"""
        results = []
        try:
            if torch.cuda.is_available():
                before_memory = torch.cuda.memory_allocated() / 1024**3
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                after_memory = torch.cuda.memory_allocated() / 1024**3
                memory_freed = before_memory - after_memory
                
                results.append(f"ğŸ§¹ CUDAç¼“å­˜æ¸…ç†: é‡Šæ”¾ {max(0, memory_freed):.2f}GB")
                
                if debug_output:
                    print(f"ğŸ§¹ CUDAç¼“å­˜æ¸…ç†å®Œæˆ: {memory_freed:.2f}GB")
            else:
                results.append("â„¹ï¸ æ— CUDAè®¾å¤‡ï¼Œè·³è¿‡ç¼“å­˜æ¸…ç†")
                
        except Exception as e:
            results.append(f"âš ï¸ CUDAç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
            if debug_output:
                print(f"âš ï¸ CUDAç¼“å­˜æ¸…ç†é”™è¯¯: {str(e)}")
        
        return results

    def get_memory_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = []
        
        # GPU å†…å­˜ç»Ÿè®¡
        if torch.cuda.is_available():
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                max_allocated = torch.cuda.max_memory_allocated() / 1024**3
                
                stats.append(f"ğŸ® GPUæ˜¾å­˜: {allocated:.2f}GB / {reserved:.2f}GB")
                stats.append(f"ğŸ“ˆ GPUå³°å€¼: {max_allocated:.2f}GB")
                
            except Exception as e:
                stats.append(f"âŒ GPUç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        # ç³»ç»Ÿå†…å­˜ç»Ÿè®¡
        try:
            import psutil
            virtual_memory = psutil.virtual_memory()
            process = psutil.Process()
            
            system_used = virtual_memory.used / 1024**3
            system_total = virtual_memory.total / 1024**3
            process_memory = process.memory_info().rss / 1024**3
            
            stats.append(f"ğŸ’» ç³»ç»Ÿå†…å­˜: {system_used:.1f}GB / {system_total:.1f}GB")
            stats.append(f"ğŸ è¿›ç¨‹å†…å­˜: {process_memory:.2f}GB")
            
        except ImportError:
            stats.append("â„¹ï¸ éœ€è¦psutilåº“è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯")
        except Exception as e:
            stats.append(f"âŒ ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        return stats

    def calculate_memory_saved(self, initial_stats, final_stats):
        """è®¡ç®—é‡Šæ”¾çš„å†…å­˜å¤§å°"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è®¡ç®—ï¼Œå®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        return "çº¦ 1-3GB (ä¼°ç®—å€¼)"

class SmartMemoryManager:
    """æ™ºèƒ½å†…å­˜ç®¡ç†å™¨ - è‡ªåŠ¨ç®¡ç†æ¨¡å‹å†…å­˜ä½¿ç”¨"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "auto_manage": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨è‡ªåŠ¨å†…å­˜ç®¡ç†\n\nğŸ¤– åŠŸèƒ½ï¼š\nâ€¢ ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ\nâ€¢ è‡ªåŠ¨å¸è½½ä¸æ´»è·ƒæ¨¡å‹\nâ€¢ é¢„é˜²æ˜¾å­˜æº¢å‡º\nâ€¢ ä¼˜åŒ–æ¨¡å‹åŠ è½½é¡ºåº"
                }),
                "memory_threshold_gb": ("FLOAT", {
                    "default": 2.0,
                    "min": 0.5,
                    "max": 8.0,
                    "step": 0.1,
                    "tooltip": "å†…å­˜è­¦æˆ’é˜ˆå€¼ (GB)\n\nâš ï¸ è®¾ç½®ï¼š\nâ€¢ å½“å¯ç”¨æ˜¾å­˜ä½äºæ­¤å€¼æ—¶è§¦å‘ç®¡ç†\nâ€¢ å€¼è¶Šå°è¶Šæ•æ„Ÿ\nâ€¢ å€¼è¶Šå¤§è¶Šä¿å®ˆ\n\nğŸ’¡ å»ºè®®ï¼š\nâ€¢ 4GBæ˜¾å­˜: 1.0-1.5GB\nâ€¢ 8GBæ˜¾å­˜: 1.5-2.5GB\nâ€¢ 12GB+æ˜¾å­˜: 2.0-4.0GB"
                }),
                "aggressiveness": (["low", "medium", "high"], {
                    "default": "medium",
                    "tooltip": "ç®¡ç†ç§¯æç¨‹åº¦\n\nğŸ¯ çº§åˆ«ï¼š\nâ€¢ low - ä½ï¼šåªåœ¨å¿…è¦æ—¶ç®¡ç†ï¼Œå½±å“æœ€å°\nâ€¢ medium - ä¸­ï¼šå¹³è¡¡ç®¡ç†å’Œæ€§èƒ½\nâ€¢ high - é«˜ï¼šç§¯æç®¡ç†ï¼Œæœ€å¤§å†…å­˜èŠ‚çœ"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("management_report", "recommendations")
    FUNCTION = "manage_memory"
    CATEGORY = "MISLG Tools/Model Management"
    DESCRIPTION = "æ™ºèƒ½å†…å­˜ç®¡ç†å™¨\n\nè‡ªåŠ¨ç›‘æ§å’Œç®¡ç†æ¨¡å‹å†…å­˜ä½¿ç”¨ï¼Œé¢„é˜²æ˜¾å­˜æº¢å‡º"

    def manage_memory(self, auto_manage, memory_threshold_gb, aggressiveness):
        report_lines = ["ğŸ¤– æ™ºèƒ½å†…å­˜ç®¡ç†æŠ¥å‘Š:"]
        recommendation_lines = ["ğŸ’¡ ä¼˜åŒ–å»ºè®®:"]
        
        if not auto_manage:
            report_lines.append("ğŸ”„ è‡ªåŠ¨ç®¡ç†å·²ç¦ç”¨")
            return ("\n".join(report_lines), "æ— å»ºè®®")
        
        try:
            # æ£€æŸ¥å½“å‰å†…å­˜çŠ¶æ€
            memory_status = self.check_memory_status()
            report_lines.extend(memory_status)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç®¡ç†
            needs_management, reason = self.needs_memory_management(memory_threshold_gb)
            
            if needs_management:
                report_lines.append(f"âš ï¸ éœ€è¦å†…å­˜ç®¡ç†: {reason}")
                
                # æ‰§è¡Œå†…å­˜ç®¡ç†
                management_results = self.execute_memory_management(aggressiveness)
                report_lines.extend(management_results)
                
                # ç”Ÿæˆå»ºè®®
                recommendations = self.generate_recommendations(aggressiveness)
                recommendation_lines.extend(recommendations)
                
            else:
                report_lines.append("âœ… å†…å­˜çŠ¶æ€è‰¯å¥½")
                recommendation_lines.append("â€¢ ç»§ç»­ä¿æŒå½“å‰è®¾ç½®")
                recommendation_lines.append("â€¢ å®šæœŸç›‘æ§å†…å­˜ä½¿ç”¨")
            
        except Exception as e:
            report_lines.append(f"âŒ å†…å­˜ç®¡ç†å¤±è´¥: {str(e)}")
            recommendation_lines.append("â€¢ æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
            recommendation_lines.append("â€¢ é‡å¯ComfyUI")
        
        management_report = "\n".join(report_lines)
        recommendations = "\n".join(recommendation_lines)
        
        return (management_report, recommendations)

    def check_memory_status(self):
        """æ£€æŸ¥å†…å­˜çŠ¶æ€"""
        status = []
        
        if torch.cuda.is_available():
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                available = torch.cuda.get_device_properties(0).total_memory / 1024**3 - reserved
                
                status.append(f"ğŸ“Š æ˜¾å­˜çŠ¶æ€:")
                status.append(f"  â€¢ å·²ä½¿ç”¨: {allocated:.2f}GB")
                status.append(f"  â€¢ å·²ä¿ç•™: {reserved:.2f}GB") 
                status.append(f"  â€¢ å¯ç”¨: {available:.2f}GB")
                
                usage_percent = (allocated / (allocated + available)) * 100
                status.append(f"  â€¢ ä½¿ç”¨ç‡: {usage_percent:.1f}%")
                
            except Exception as e:
                status.append(f"âŒ æ˜¾å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        return status

    def needs_memory_management(self, threshold_gb):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å†…å­˜ç®¡ç†"""
        if not torch.cuda.is_available():
            return False, "æ— CUDAè®¾å¤‡"
        
        try:
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            available = total - reserved
            
            if available < threshold_gb:
                return True, f"å¯ç”¨æ˜¾å­˜ä¸è¶³ ({available:.2f}GB < {threshold_gb}GB)"
            
            usage_percent = (allocated / total) * 100
            if usage_percent > 85:
                return True, f"æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({usage_percent:.1f}%)"
            
            return False, "å†…å­˜çŠ¶æ€æ­£å¸¸"
            
        except Exception as e:
            return True, f"æ£€æŸ¥å¤±è´¥: {str(e)}"

    def execute_memory_management(self, aggressiveness):
        """æ‰§è¡Œå†…å­˜ç®¡ç†"""
        results = []
        
        # æ ¹æ®ç§¯æç¨‹åº¦æ‰§è¡Œä¸åŒçš„ç®¡ç†ç­–ç•¥
        if aggressiveness == "low":
            results.append("ğŸ”§ æ‰§è¡Œè½»åº¦å†…å­˜ç®¡ç†")
            # åªæ¸…ç†ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("âœ… æ¸…ç†GPUç¼“å­˜")
                
        elif aggressiveness == "medium":
            results.append("ğŸ”§ æ‰§è¡Œä¸­åº¦å†…å­˜ç®¡ç†")
            # æ¸…ç†ç¼“å­˜å’Œåƒåœ¾å›æ”¶
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("âœ… æ¸…ç†GPUç¼“å­˜")
            
            gc.collect()
            results.append("âœ… æ‰§è¡Œåƒåœ¾å›æ”¶")
            
        else:  # high
            results.append("ğŸ”§ æ‰§è¡Œç§¯æå†…å­˜ç®¡ç†")
            # å…¨é¢æ¸…ç†
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("âœ… æ¸…ç†GPUç¼“å­˜")
            
            # å¤šä»£åƒåœ¾å›æ”¶
            gc.collect(0)
            gc.collect(1)
            gc.collect(2)
            results.append("âœ… æ·±åº¦åƒåœ¾å›æ”¶")
        
        return results

    def generate_recommendations(self, aggressiveness):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if aggressiveness == "low":
            recommendations.extend([
                "â€¢ è€ƒè™‘æé«˜ç®¡ç†ç§¯æç¨‹åº¦",
                "â€¢ æ‰‹åŠ¨å¸è½½ä¸ä½¿ç”¨çš„æ¨¡å‹", 
                "â€¢ ä½¿ç”¨åˆ†å—å¤„ç†å¤§å›¾åƒ",
                "â€¢ å…³é—­ä¸å¿…è¦çš„é¢„è§ˆ"
            ])
        elif aggressiveness == "medium":
            recommendations.extend([
                "â€¢ å½“å‰è®¾ç½®å¹³è¡¡è‰¯å¥½",
                "â€¢ å¯å°è¯•ä½¿ç”¨é€šç”¨æ¨¡å‹å¸è½½å™¨",
                "â€¢ è€ƒè™‘ä¼˜åŒ–å·¥ä½œæµç»“æ„",
                "â€¢ å®šæœŸé‡å¯ComfyUIé‡Šæ”¾å†…å­˜"
            ])
        else:  # high
            recommendations.extend([
                "â€¢ ç§¯æç®¡ç†å·²å¯ç”¨",
                "â€¢ è€ƒè™‘é™ä½å›¾åƒåˆ†è¾¨ç‡",
                "â€¢ ä½¿ç”¨æ›´å°çš„æ¨¡å‹å°ºå¯¸",
                "â€¢ åˆ†æ‰¹å¤„ç†å¤æ‚å·¥ä½œæµ"
            ])
        
        return recommendations

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "UniversalModelUnloader": UniversalModelUnloader,
    "SmartMemoryManager": SmartMemoryManager,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "UniversalModelUnloader": "ğŸ’¾ é€šç”¨æ¨¡å‹å¸è½½å™¨",
    "SmartMemoryManager": "ğŸ¤– æ™ºèƒ½å†…å­˜ç®¡ç†å™¨",
}