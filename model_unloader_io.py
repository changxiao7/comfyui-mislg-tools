"""
é€šç”¨æ¨¡å‹å¸è½½èŠ‚ç‚¹ - åŸºäºComfyUIå†…éƒ¨APIçš„é«˜æ•ˆç‰ˆæœ¬
å®Œæ•´æ”¯æŒæ‰€æœ‰ComfyUIæ•°æ®ç±»å‹
"""

import comfy.model_management as model_management
import gc
import torch
import time
import psutil
from typing import Any, Dict, List, Tuple

class AnyType(str):
    """é€šé…ç¬¦ç±»å‹ï¼Œç”¨äºæ”¯æŒä»»æ„è¾“å…¥ç±»å‹"""
    def __ne__(self, __value: object) -> bool:
        return False

any = AnyType("*")

class UniversalModelUnloaderWithIO:
    """é€šç”¨æ¨¡å‹å¸è½½å™¨ - åŸºäºComfyUIå†…éƒ¨APIçš„é«˜æ•ˆç‰ˆæœ¬"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "trigger_unload": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "è§¦å‘æ¨¡å‹å¸è½½æ“ä½œ"
                }),
                "unload_mode": (["specific", "all_models", "aggressive"], {
                    "default": "specific",
                    "tooltip": "å¸è½½æ¨¡å¼\nâ€¢ specific: å¸è½½æŒ‡å®šç±»å‹æ¨¡å‹\nâ€¢ all_models: å¸è½½æ‰€æœ‰æ¨¡å‹\nâ€¢ aggressive: å¼ºåˆ¶æ·±åº¦æ¸…ç†"
                }),
                # æ¨¡å‹ç±»å‹é€‰æ‹©å¼€å…³
                "unload_vae": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ VAE æ¨¡å‹"
                }),
                "unload_clip": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ CLIP æ¨¡å‹"
                }),
                "unload_unet": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ UNet æ¨¡å‹"
                }),
                "unload_controlnet": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ ControlNet æ¨¡å‹"
                }),
                "debug_output": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨è°ƒè¯•è¾“å‡º"
                }),
            },
            "optional": {
                "image_input": ("IMAGE",),
                "latent_input": ("LATENT",),
                "conditioning_input": ("CONDITIONING",),
                "vae_input": ("VAE",),
                "clip_input": ("CLIP",),
                "model_input": ("MODEL",),
                "controlnet_input": ("CONTROL_NET",),
                "upscale_input": ("UPSCALE_MODEL",),
                "any_input": (any,),  # é€šé…ç¬¦è¾“å…¥ï¼Œæ”¯æŒä»»æ„ç±»å‹
            }
        }
    
    RETURN_TYPES = ("IMAGE", "LATENT", "CONDITIONING", "VAE", "CLIP", "MODEL", "CONTROL_NET", "UPSCALE_MODEL", any, "STRING", "STRING")
    RETURN_NAMES = ("image_out", "latent_out", "conditioning_out", "vae_out", "clip_out", "model_out", "controlnet_out", "upscale_out", "any_out", "unload_report", "memory_stats")
    FUNCTION = "unload_models"
    CATEGORY = "MISLG Tools/Model Management"
    DESCRIPTION = "é€šç”¨æ¨¡å‹å¸è½½å™¨ - åŸºäºComfyUIå†…éƒ¨APIçš„é«˜æ•ˆç‰ˆæœ¬"

    def unload_models(self, 
                     trigger_unload: bool = True,
                     unload_mode: str = "specific",
                     unload_vae: bool = True,
                     unload_clip: bool = True,
                     unload_unet: bool = True,
                     unload_controlnet: bool = True,
                     debug_output: bool = False,
                     **kwargs):
        
        if not trigger_unload:
            return self._return_passthrough(kwargs, "ğŸ”„ å¸è½½æ“ä½œæœªè§¦å‘", "æ— æ“ä½œ")
        
        report_lines = ["ğŸš€ å¼€å§‹æ¨¡å‹å¸è½½æ“ä½œ"]
        memory_lines = ["ğŸ“Š å†…å­˜ç»Ÿè®¡:"]
        
        # è®°å½•åˆå§‹å†…å­˜çŠ¶æ€
        initial_stats = self.get_memory_stats()
        memory_lines.extend(initial_stats)
        
        try:
            # æ‰§è¡Œæ¨¡å‹å¸è½½
            if unload_mode == "all_models":
                unload_results = self.unload_all_models(debug_output)
            elif unload_mode == "aggressive":
                unload_results = self.aggressive_unload(debug_output)
            else:  # specific mode
                unload_results = self.unload_specific_models(
                    unload_vae, unload_clip, unload_unet, unload_controlnet, 
                    kwargs, debug_output
                )
            
            report_lines.extend(unload_results)
            
            # è®°å½•æœ€ç»ˆå†…å­˜çŠ¶æ€
            final_stats = self.get_memory_stats()
            memory_saved = self.calculate_memory_saved(initial_stats, final_stats)
            
            memory_lines.extend(final_stats)
            memory_lines.append(f"ğŸ’¾ æ€»è®¡é‡Šæ”¾: {memory_saved}")
            
            report_lines.append("âœ… æ¨¡å‹å¸è½½å®Œæˆ")
            
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"
            report_lines.append(error_msg)
            if debug_output:
                print(f"âŒ å¸è½½é”™è¯¯: {str(e)}")
        
        return self._return_passthrough(kwargs, "\n".join(report_lines), "\n".join(memory_lines))

    def unload_specific_models(self, unload_vae, unload_clip, unload_unet, unload_controlnet, inputs, debug_output):
        """å¸è½½æŒ‡å®šç±»å‹çš„æ¨¡å‹"""
        results = []
        models_unloaded = 0
        
        # ä½¿ç”¨ComfyUIçš„å†…éƒ¨APIå¸è½½æ¨¡å‹
        loaded_models = model_management.loaded_models()
        
        # å¸è½½ä¼ å…¥çš„ç‰¹å®šæ¨¡å‹
        if unload_vae and inputs.get("vae_input") is not None:
            vae_model = inputs.get("vae_input")
            if vae_model in loaded_models:
                loaded_models.remove(vae_model)
                models_unloaded += 1
                results.append("âœ… å¸è½½ VAE æ¨¡å‹")
                if debug_output:
                    print(" - VAEæ¨¡å‹ä»å†…å­˜ä¸­ç§»é™¤")
        
        if unload_clip and inputs.get("clip_input") is not None:
            clip_model = inputs.get("clip_input")
            if clip_model in loaded_models:
                loaded_models.remove(clip_model)
                models_unloaded += 1
                results.append("âœ… å¸è½½ CLIP æ¨¡å‹")
                if debug_output:
                    print(" - CLIPæ¨¡å‹ä»å†…å­˜ä¸­ç§»é™¤")
        
        if unload_unet and inputs.get("model_input") is not None:
            unet_model = inputs.get("model_input")
            if unet_model in loaded_models:
                loaded_models.remove(unet_model)
                models_unloaded += 1
                results.append("âœ… å¸è½½ UNet æ¨¡å‹")
                if debug_output:
                    print(" - UNetæ¨¡å‹ä»å†…å­˜ä¸­ç§»é™¤")
        
        if unload_controlnet and inputs.get("controlnet_input") is not None:
            controlnet_model = inputs.get("controlnet_input")
            if controlnet_model in loaded_models:
                loaded_models.remove(controlnet_model)
                models_unloaded += 1
                results.append("âœ… å¸è½½ ControlNet æ¨¡å‹")
                if debug_output:
                    print(" - ControlNetæ¨¡å‹ä»å†…å­˜ä¸­ç§»é™¤")
        
        # å¼ºåˆ¶é‡Šæ”¾å†…å­˜
        if models_unloaded > 0:
            model_management.free_memory(1e30, model_management.get_torch_device(), loaded_models)
            model_management.soft_empty_cache(True)
            
            # æ¸…ç†ç¼“å­˜
            try:
                gc.collect()
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                if debug_output:
                    print(" - ç¼“å­˜æ¸…ç†å®Œæˆ")
            except Exception as e:
                if debug_output:
                    print(f"   - ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
        
        if models_unloaded == 0:
            results.append("â„¹ï¸ æœªæ‰¾åˆ°å¯å¸è½½çš„æŒ‡å®šæ¨¡å‹")
        
        results.append(f"ğŸ“¦ æ€»è®¡å¸è½½: {models_unloaded} ä¸ªæ¨¡å‹")
        
        return results

    def unload_all_models(self, debug_output):
        """å¸è½½æ‰€æœ‰æ¨¡å‹"""
        results = []
        
        if debug_output:
            print(" - å¸è½½æ‰€æœ‰æ¨¡å‹...")
        
        # ä½¿ç”¨ComfyUIçš„å†…éƒ¨APIå¸è½½æ‰€æœ‰æ¨¡å‹
        model_management.unload_all_models()
        model_management.soft_empty_cache(True)
        
        # æ·±åº¦æ¸…ç†ç¼“å­˜
        try:
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            if debug_output:
                print(" - æ·±åº¦ç¼“å­˜æ¸…ç†å®Œæˆ")
        except Exception as e:
            if debug_output:
                print(f"   - ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
        
        results.append("âœ… å¸è½½æ‰€æœ‰æ¨¡å‹")
        results.append("ğŸ§¹ æ‰§è¡Œæ·±åº¦ç¼“å­˜æ¸…ç†")
        
        return results

    def aggressive_unload(self, debug_output):
        """æ¿€è¿›æ¨¡å¼å¸è½½ - æœ€å¤§ç¨‹åº¦é‡Šæ”¾å†…å­˜"""
        results = []
        
        if debug_output:
            print(" - æ‰§è¡Œæ¿€è¿›æ¨¡å¼å¸è½½...")
        
        # å¤šæ¬¡å¸è½½ç¡®ä¿å½»åº•æ¸…ç†
        for i in range(2):
            model_management.unload_all_models()
            model_management.soft_empty_cache(True)
            
            if debug_output:
                print(f" - ç¬¬ {i+1} è½®å¸è½½å®Œæˆ")
        
        # å¤šæ¬¡æ¸…ç†ç¼“å­˜
        for i in range(3):
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        
        if debug_output:
            print(" - æ¿€è¿›æ¨¡å¼å¸è½½å®Œæˆ")
        
        results.append("ğŸ’¥ æ¿€è¿›æ¨¡å¼å¸è½½å®Œæˆ")
        results.append("ğŸ” æ‰§è¡Œå¤šè½®æ·±åº¦æ¸…ç†")
        results.append("ğŸ§¹ å½»åº•é‡Šæ”¾GPUå†…å­˜")
        
        return results

    def _return_passthrough(self, inputs, report, stats):
        """è¿”å›ä¼ é€’çš„æ•°æ®å’ŒæŠ¥å‘Š"""
        return (
            inputs.get("image_input"),      # IMAGE
            inputs.get("latent_input"),     # LATENT
            inputs.get("conditioning_input"), # CONDITIONING
            inputs.get("vae_input"),        # VAE
            inputs.get("clip_input"),       # CLIP
            inputs.get("model_input"),      # MODEL
            inputs.get("controlnet_input"), # CONTROL_NET
            inputs.get("upscale_input"),    # UPSCALE_MODEL
            inputs.get("any_input"),        # any
            report,                         # STRING
            stats                           # STRING
        )

    def get_memory_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = []
        
        # GPU å†…å­˜ç»Ÿè®¡
        if torch.cuda.is_available():
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                
                stats.append(f"ğŸ® GPUæ˜¾å­˜: {allocated:.2f}GB / {reserved:.2f}GB")
                
            except Exception as e:
                stats.append(f"âŒ GPUç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        # ç³»ç»Ÿå†…å­˜ç»Ÿè®¡
        try:
            import psutil
            virtual_memory = psutil.virtual_memory()
            process = psutil.Process()
            
            system_used = virtual_memory.used / 1024**3
            system_total = virtual_memory.total / 1024**3
            process_memory = process.memory_info().rss / 1024**3
            
            stats.append(f"ğŸ’» ç³»ç»Ÿå†…å­˜: {system_used:.1f}GB / {system_total:.1f}GB")
            stats.append(f"ğŸ è¿›ç¨‹å†…å­˜: {process_memory:.2f}GB")
            
        except ImportError:
            stats.append("â„¹ï¸ éœ€è¦psutilåº“è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯")
        except Exception as e:
            stats.append(f"âŒ ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        return stats

    def calculate_memory_saved(self, initial_stats, final_stats):
        """è®¡ç®—é‡Šæ”¾çš„å†…å­˜å¤§å°"""
        # ç®€åŒ–è®¡ç®—ï¼Œå®é™…å€¼é€šè¿‡å†…å­˜ç»Ÿè®¡æ˜¾ç¤º
        return "é€šè¿‡å†…å­˜ç»Ÿè®¡æŸ¥çœ‹å…·ä½“é‡Šæ”¾é‡"

class SmartModelManager:
    """æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨ - åŸºäºComfyUIå†…éƒ¨API"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "auto_manage": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨è‡ªåŠ¨å†…å­˜ç®¡ç†"
                }),
                "memory_threshold_gb": ("FLOAT", {
                    "default": 2.0,
                    "min": 0.5,
                    "max": 8.0,
                    "step": 0.1,
                    "tooltip": "å†…å­˜è­¦æˆ’é˜ˆå€¼ (GB)"
                }),
                "auto_unload_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "è‡ªåŠ¨å¸è½½ä¸æ´»è·ƒæ¨¡å‹"
                }),
                "debug_output": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨è°ƒè¯•è¾“å‡º"
                }),
            },
            "optional": {
                "image_input": ("IMAGE",),
                "latent_input": ("LATENT",),
                "conditioning_input": ("CONDITIONING",),
                "vae_input": ("VAE",),
                "clip_input": ("CLIP",),
                "model_input": ("MODEL",),
                "controlnet_input": ("CONTROL_NET",),
                "upscale_input": ("UPSCALE_MODEL",),
                "any_input": (any,),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "LATENT", "CONDITIONING", "VAE", "CLIP", "MODEL", "CONTROL_NET", "UPSCALE_MODEL", any, "STRING", "STRING")
    RETURN_NAMES = ("image_out", "latent_out", "conditioning_out", "vae_out", "clip_out", "model_out", "controlnet_out", "upscale_out", "any_out", "management_report", "recommendations")
    FUNCTION = "manage_memory"
    CATEGORY = "MISLG Tools/Model Management"
    DESCRIPTION = "æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨ - åŸºäºComfyUIå†…éƒ¨API"

    def manage_memory(self, 
                     auto_manage: bool = True,
                     memory_threshold_gb: float = 2.0,
                     auto_unload_models: bool = True,
                     debug_output: bool = False,
                     **kwargs):
        
        if not auto_manage:
            return self._return_passthrough(kwargs, "ğŸ”„ è‡ªåŠ¨ç®¡ç†å·²ç¦ç”¨", "æ— å»ºè®®")
        
        report_lines = ["ğŸ¤– æ™ºèƒ½å†…å­˜ç®¡ç†æŠ¥å‘Š:"]
        recommendation_lines = ["ğŸ’¡ ä¼˜åŒ–å»ºè®®:"]
        
        try:
            # æ£€æŸ¥å†…å­˜çŠ¶æ€
            memory_status = self.check_memory_status()
            report_lines.extend(memory_status)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç®¡ç†
            needs_management, reason = self.needs_memory_management(memory_threshold_gb)
            
            if needs_management:
                report_lines.append(f"âš ï¸ éœ€è¦å†…å­˜ç®¡ç†: {reason}")
                
                # æ‰§è¡Œå†…å­˜ç®¡ç†
                if auto_unload_models:
                    management_results = self.execute_auto_management(debug_output)
                    report_lines.extend(management_results)
                else:
                    report_lines.append("â„¹ï¸ è‡ªåŠ¨å¸è½½å·²ç¦ç”¨ï¼Œä»…è¿›è¡Œç›‘æ§")
                
                # ç”Ÿæˆå»ºè®®
                recommendations = self.generate_recommendations(memory_threshold_gb)
                recommendation_lines.extend(recommendations)
                
            else:
                report_lines.append("âœ… å†…å­˜çŠ¶æ€è‰¯å¥½")
                recommendation_lines.append("â€¢ ç»§ç»­ä¿æŒå½“å‰è®¾ç½®")
                
        except Exception as e:
            report_lines.append(f"âŒ å†…å­˜ç®¡ç†å¤±è´¥: {str(e)}")
            recommendation_lines.append("â€¢ æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
        
        return self._return_passthrough(kwargs, "\n".join(report_lines), "\n".join(recommendation_lines))

    def execute_auto_management(self, debug_output):
        """æ‰§è¡Œè‡ªåŠ¨å†…å­˜ç®¡ç†"""
        results = []
        
        if debug_output:
            print("ğŸ¤– æ‰§è¡Œè‡ªåŠ¨å†…å­˜ç®¡ç†...")
        
        # ä½¿ç”¨ComfyUIçš„å†…éƒ¨APIè¿›è¡Œå†…å­˜ç®¡ç†
        model_management.free_memory(1e30, model_management.get_torch_device())
        model_management.soft_empty_cache(True)
        
        # æ¸…ç†ç¼“å­˜
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        
        results.append("âœ… è‡ªåŠ¨å†…å­˜ç®¡ç†å®Œæˆ")
        results.append("ğŸ§¹ æ¸…ç†ä¸æ´»è·ƒæ¨¡å‹")
        
        return results

    def check_memory_status(self):
        """æ£€æŸ¥å†…å­˜çŠ¶æ€"""
        status = []
        
        if torch.cuda.is_available():
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                available = total - reserved
                
                status.append(f"ğŸ“Š æ˜¾å­˜çŠ¶æ€:")
                status.append(f"  â€¢ å·²ä½¿ç”¨: {allocated:.2f}GB")
                status.append(f"  â€¢ å·²ä¿ç•™: {reserved:.2f}GB") 
                status.append(f"  â€¢ å¯ç”¨: {available:.2f}GB")
                status.append(f"  â€¢ ä½¿ç”¨ç‡: {(allocated/total)*100:.1f}%")
                
            except Exception as e:
                status.append(f"âŒ æ˜¾å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        return status

    def needs_memory_management(self, threshold_gb):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å†…å­˜ç®¡ç†"""
        if not torch.cuda.is_available():
            return False, "æ— CUDAè®¾å¤‡"
        
        try:
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            available = total - reserved
            
            if available < threshold_gb:
                return True, f"å¯ç”¨æ˜¾å­˜ä¸è¶³ ({available:.2f}GB < {threshold_gb}GB)"
            
            usage_percent = (allocated / total) * 100
            if usage_percent > 85:
                return True, f"æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({usage_percent:.1f}%)"
            
            return False, "å†…å­˜çŠ¶æ€æ­£å¸¸"
            
        except Exception as e:
            return True, f"æ£€æŸ¥å¤±è´¥: {str(e)}"

    def generate_recommendations(self, threshold):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if threshold < 1.5:
            recommendations.extend([
                "â€¢ è€ƒè™‘æé«˜å†…å­˜é˜ˆå€¼ä»¥å‡å°‘é¢‘ç¹ç®¡ç†",
                "â€¢ ä¼˜åŒ–å·¥ä½œæµå‡å°‘å†…å­˜ä½¿ç”¨",
                "â€¢ ä½¿ç”¨æ›´å°çš„æ¨¡å‹å°ºå¯¸"
            ])
        elif threshold > 3.0:
            recommendations.extend([
                "â€¢ å½“å‰é˜ˆå€¼è®¾ç½®è¾ƒä¸ºå®½æ¾",
                "â€¢ å¯é€‚å½“é™ä½é˜ˆå€¼ä»¥æ›´ç§¯æç®¡ç†",
                "â€¢ ç›‘æ§å†…å­˜ä½¿ç”¨æ¨¡å¼"
            ])
        else:
            recommendations.extend([
                "â€¢ å½“å‰è®¾ç½®å¹³è¡¡è‰¯å¥½",
                "â€¢ ç»§ç»­ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ",
                "â€¢ æ ¹æ®éœ€è¦è°ƒæ•´é˜ˆå€¼"
            ])
        
        return recommendations

    def _return_passthrough(self, inputs, report, recommendations):
        """è¿”å›ä¼ é€’çš„æ•°æ®å’ŒæŠ¥å‘Š"""
        return (
            inputs.get("image_input"),      # IMAGE
            inputs.get("latent_input"),     # LATENT
            inputs.get("conditioning_input"), # CONDITIONING
            inputs.get("vae_input"),        # VAE
            inputs.get("clip_input"),       # CLIP
            inputs.get("model_input"),      # MODEL
            inputs.get("controlnet_input"), # CONTROL_NET
            inputs.get("upscale_input"),    # UPSCALE_MODEL
            inputs.get("any_input"),        # any
            report,                         # STRING
            recommendations                 # STRING
        )

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "UniversalModelUnloaderWithIO": UniversalModelUnloaderWithIO,
    "SmartModelManager": SmartModelManager,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "UniversalModelUnloaderWithIO": "ğŸ’¾ é€šç”¨æ¨¡å‹å¸è½½å™¨ (é«˜æ•ˆç‰ˆ)",
    "SmartModelManager": "ğŸ¤– æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨",
}