"""
é€šç”¨æ¨¡å‹å¸è½½èŠ‚ç‚¹ - å¸¦å®Œæ•´è¾“å…¥è¾“å‡ºæ¥å£å’Œæ¨¡å‹é€‰æ‹©å¼€å…³
æ”¯æŒåœ¨å·¥ä½œæµä¸­ä»»æ„ä½ç½®æ’å…¥ï¼Œä¸ä¸­æ–­æ•°æ®æµ
"""

import torch
import gc
import psutil
import os
from typing import Any, Dict, List, Tuple

class UniversalModelUnloaderWithIO:
    """é€šç”¨æ¨¡å‹å¸è½½å™¨ - å¸¦å®Œæ•´è¾“å…¥è¾“å‡ºæ¥å£å’Œæ¨¡å‹é€‰æ‹©å¼€å…³"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "trigger_unload": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "è§¦å‘æ¨¡å‹å¸è½½æ“ä½œ\n\nğŸ¯ åŠŸèƒ½ï¼š\nâ€¢ æ‰§è¡Œæ¨¡å‹å¸è½½æµç¨‹\nâ€¢ é‡Šæ”¾æ˜¾å­˜å’Œå†…å­˜\nâ€¢ æ¸…ç†æ¨¡å‹ç¼“å­˜\n\nğŸ’¡ ä½¿ç”¨ï¼š\nâ€¢ è®¾ç½®ä¸ºTrueæ‰§è¡Œå¸è½½\nâ€¢ è®¾ç½®ä¸ºFalseè·³è¿‡å¸è½½"
                }),
                "unload_mode": (["aggressive", "balanced", "conservative"], {
                    "default": "balanced",
                    "tooltip": "æ¨¡å‹å¸è½½æ¨¡å¼\n\nğŸ”§ æ¨¡å¼è¯´æ˜ï¼š\nâ€¢ aggressive - æ¿€è¿›æ¨¡å¼ï¼šå¼ºåˆ¶å¸è½½æ‰€æœ‰æ¨¡å‹ï¼Œæœ€å¤§æ˜¾å­˜é‡Šæ”¾\nâ€¢ balanced - å¹³è¡¡æ¨¡å¼ï¼šæ™ºèƒ½å¸è½½ï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜\nâ€¢ conservative - ä¿å®ˆæ¨¡å¼ï¼šåªå¸è½½ä¸æ´»è·ƒæ¨¡å‹ï¼Œæœ€å°å½±å“å·¥ä½œæµ\n\nğŸ“Œ å»ºè®®ï¼š\nâ€¢ æ‰¹å¤„ç†ï¼šä½¿ç”¨aggressive\nâ€¢ å¸¸è§„ä½¿ç”¨ï¼šä½¿ç”¨balanced\nâ€¢ è°ƒè¯•ï¼šä½¿ç”¨conservative"
                }),
                "force_garbage_collect": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¼ºåˆ¶åƒåœ¾å›æ”¶\n\nğŸ—‘ï¸ åŠŸèƒ½ï¼š\nâ€¢ æ‰§è¡ŒPythonåƒåœ¾å›æ”¶\nâ€¢ æ¸…ç†æ‰€æœ‰ç¼“å­˜\nâ€¢ é‡Šæ”¾æœªä½¿ç”¨å†…å­˜\n\nâœ… å»ºè®®ï¼š\nâ€¢ é€šå¸¸ä¿æŒå¯ç”¨\nâ€¢ å¦‚æœé‡åˆ°æ€§èƒ½é—®é¢˜å¯å…³é—­"
                }),
                "clear_cuda_cache": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ¸…ç†CUDAç¼“å­˜\n\nğŸ§¹ åŠŸèƒ½ï¼š\nâ€¢ æ¸…ç©ºGPUç¼“å­˜\nâ€¢ é‡ç½®CUDAå†…å­˜åˆ†é…å™¨\nâ€¢ é‡Šæ”¾ç¢ç‰‡åŒ–æ˜¾å­˜\n\nâš ï¸ æ³¨æ„ï¼š\nâ€¢ å¯èƒ½ä¼šæš‚æ—¶å½±å“æ€§èƒ½\nâ€¢ ä½†èƒ½æœ‰æ•ˆè§£å†³æ˜¾å­˜ç¢ç‰‡"
                }),
                # æ¨¡å‹ç±»å‹é€‰æ‹©å¼€å…³
                "unload_vae_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ VAE æ¨¡å‹\n\nğŸ¨ å½±å“ï¼š\nâ€¢ VAEè§£ç å™¨æ¨¡å‹\nâ€¢ å›¾åƒç¼–ç å™¨/è§£ç å™¨\nâ€¢ é¢œè‰²è½¬æ¢æ¨¡å‹"
                }),
                "unload_clip_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ CLIP æ¨¡å‹\n\nğŸ“ å½±å“ï¼š\nâ€¢ æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹\nâ€¢ æ–‡æœ¬ç†è§£æ¨¡å‹\nâ€¢ è¯­ä¹‰åˆ†ææ¨¡å‹"
                }),
                "unload_unet_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ UNet æ¨¡å‹\n\nğŸ”„ å½±å“ï¼š\nâ€¢ æ‰©æ•£æ¨¡å‹ä¸»å¹²\nâ€¢ å›¾åƒç”Ÿæˆæ¨¡å‹\nâ€¢ å™ªå£°é¢„æµ‹å™¨"
                }),
                "unload_controlnet_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½ ControlNet æ¨¡å‹\nğŸ›ï¸ å½±å“ï¼š\nâ€¢ æ§åˆ¶ç½‘ç»œæ¨¡å‹\nâ€¢ æ¡ä»¶ç”Ÿæˆæ¨¡å‹\nâ€¢ å§¿æ€/è¾¹ç¼˜æ£€æµ‹æ¨¡å‹"
                }),
                "unload_upscale_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½è¶…åˆ†æ¨¡å‹\n\nğŸ” å½±å“ï¼š\nâ€¢ å›¾åƒæ”¾å¤§æ¨¡å‹\nâ€¢ ç»†èŠ‚å¢å¼ºæ¨¡å‹\nâ€¢ åˆ†è¾¨ç‡æå‡æ¨¡å‹"
                }),
                "unload_face_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½äººè„¸æ¨¡å‹\n\nğŸ‘¤ å½±å“ï¼š\nâ€¢ é¢éƒ¨è¯†åˆ«æ¨¡å‹\nâ€¢ äººè„¸ä¿®å¤æ¨¡å‹\nâ€¢ ç‰¹å¾ç‚¹æ£€æµ‹æ¨¡å‹"
                }),
                "unload_segment_models": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¸è½½åˆ†å‰²æ¨¡å‹\n\nğŸ¯ å½±å“ï¼š\nâ€¢ è¯­ä¹‰åˆ†å‰²æ¨¡å‹\nâ€¢ å®ä¾‹åˆ†å‰²æ¨¡å‹\nâ€¢ è¾¹ç¼˜æ£€æµ‹æ¨¡å‹"
                }),
                "unload_depth_models": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¸è½½æ·±åº¦æ¨¡å‹\n\nğŸ“ å½±å“ï¼š\nâ€¢ æ·±åº¦ä¼°è®¡æ¨¡å‹\nâ€¢ 3Dé‡å»ºæ¨¡å‹\nâ€¢ ç«‹ä½“è§†è§‰æ¨¡å‹"
                }),
                "unload_other_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¸è½½å…¶ä»–æ¨¡å‹\n\nğŸ“¦ å½±å“ï¼š\nâ€¢ è‡ªå®šä¹‰æ¨¡å‹\nâ€¢ ç¬¬ä¸‰æ–¹æ’ä»¶æ¨¡å‹\nâ€¢ æœªåˆ†ç±»æ¨¡å‹"
                }),
                "debug_output": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨è°ƒè¯•è¾“å‡º\n\nğŸ“ åŠŸèƒ½ï¼š\nâ€¢ æ˜¾ç¤ºè¯¦ç»†å¸è½½è¿‡ç¨‹\nâ€¢ æŠ¥å‘Šé‡Šæ”¾çš„æ˜¾å­˜\nâ€¢ å¸®åŠ©è¯Šæ–­å†…å­˜é—®é¢˜\n\nğŸ”§ å»ºè®®ï¼š\nâ€¢ è°ƒè¯•æ—¶å¼€å¯\nâ€¢ æ­£å¸¸ä½¿ç”¨æ—¶å…³é—­ä»¥æå‡æ€§èƒ½"
                }),
            },
            "optional": {
                "image_input": ("IMAGE",),
                "latent_input": ("LATENT",),
                "conditioning_input": ("CONDITIONING",),
                "vae_input": ("VAE",),
                "clip_input": ("CLIP",),
                "model_input": ("MODEL",),
                "controlnet_input": ("CONTROL_NET",),
                "upscale_input": ("UPSCALE_MODEL",),
                "any_input": ("*",),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "LATENT", "CONDITIONING", "VAE", "CLIP", "MODEL", "CONTROL_NET", "UPSCALE_MODEL", "STRING", "STRING")
    RETURN_NAMES = ("image_out", "latent_out", "conditioning_out", "vae_out", "clip_out", "model_out", "controlnet_out", "upscale_out", "unload_report", "memory_stats")
    FUNCTION = "process_with_unload"
    CATEGORY = "MISLG Tools/Model Management"
    DESCRIPTION = "é€šç”¨æ¨¡å‹å¸è½½å™¨ - å¸¦å®Œæ•´è¾“å…¥è¾“å‡ºæ¥å£å’Œæ¨¡å‹é€‰æ‹©å¼€å…³ï¼Œå¯åœ¨å·¥ä½œæµä»»æ„ä½ç½®æ’å…¥"

    def process_with_unload(self, 
                          trigger_unload: bool = True,
                          unload_mode: str = "balanced",
                          force_garbage_collect: bool = True,
                          clear_cuda_cache: bool = True,
                          unload_vae_models: bool = True,
                          unload_clip_models: bool = True,
                          unload_unet_models: bool = True,
                          unload_controlnet_models: bool = True,
                          unload_upscale_models: bool = True,
                          unload_face_models: bool = True,
                          unload_segment_models: bool = False,
                          unload_depth_models: bool = False,
                          unload_other_models: bool = True,
                          debug_output: bool = False,
                          **kwargs):
        
        # ä¿å­˜è¾“å…¥æ•°æ®ä»¥ä¾¿åŸæ ·è¿”å›
        input_data = {
            "image_input": kwargs.get("image_input", None),
            "latent_input": kwargs.get("latent_input", None),
            "conditioning_input": kwargs.get("conditioning_input", None),
            "vae_input": kwargs.get("vae_input", None),
            "clip_input": kwargs.get("clip_input", None),
            "model_input": kwargs.get("model_input", None),
            "controlnet_input": kwargs.get("controlnet_input", None),
            "upscale_input": kwargs.get("upscale_input", None),
            "any_input": kwargs.get("any_input", None),
        }
        
        # æ‰§è¡Œæ¨¡å‹å¸è½½
        unload_report, memory_stats = self.execute_unload_operation(
            trigger_unload, unload_mode, force_garbage_collect, clear_cuda_cache,
            unload_vae_models, unload_clip_models, unload_unet_models,
            unload_controlnet_models, unload_upscale_models, unload_face_models,
            unload_segment_models, unload_depth_models, unload_other_models,
            debug_output
        )
        
        # è¿”å›æ‰€æœ‰è¾“å…¥æ•°æ®ï¼ˆåŸæ ·ä¼ é€’ï¼‰å’Œå¸è½½æŠ¥å‘Š
        return (
            input_data["image_input"],
            input_data["latent_input"], 
            input_data["conditioning_input"],
            input_data["vae_input"],
            input_data["clip_input"],
            input_data["model_input"],
            input_data["controlnet_input"],
            input_data["upscale_input"],
            unload_report,
            memory_stats
        )

    def execute_unload_operation(self, trigger_unload, unload_mode, force_garbage_collect, clear_cuda_cache,
                               unload_vae, unload_clip, unload_unet, unload_controlnet, 
                               unload_upscale, unload_face, unload_segment, unload_depth, unload_other, debug_output):
        """æ‰§è¡Œæ¨¡å‹å¸è½½æ“ä½œ"""
        
        if not trigger_unload:
            return ("ğŸ”„ å¸è½½æ“ä½œæœªè§¦å‘", "æ— æ“ä½œ")
        
        report_lines = ["ğŸš€ å¼€å§‹é€šç”¨æ¨¡å‹å¸è½½æ“ä½œ"]
        memory_lines = ["ğŸ“Š å†…å­˜ç»Ÿè®¡:"]
        
        # è®°å½•åˆå§‹å†…å­˜çŠ¶æ€
        initial_stats = self.get_memory_stats()
        memory_lines.extend(initial_stats)
        
        if debug_output:
            print("ğŸš€ å¼€å§‹é€šç”¨æ¨¡å‹å¸è½½...")
            print(f"ğŸ”§ å¸è½½æ¨¡å¼: {unload_mode}")
        
        try:
            # æ ¹æ®å¸è½½æ¨¡å¼è°ƒæ•´ç­–ç•¥
            unload_strategy = self.get_unload_strategy(unload_mode)
            
            # æ‰§è¡Œæ¨¡å‹å¸è½½
            unload_results = self.execute_model_unload(
                unload_strategy,
                unload_vae, unload_clip, unload_unet, unload_controlnet,
                unload_upscale, unload_face, unload_segment, unload_depth, unload_other,
                debug_output
            )
            
            report_lines.extend(unload_results)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            if force_garbage_collect:
                gc_results = self.force_garbage_collection(debug_output)
                report_lines.extend(gc_results)
            
            # æ¸…ç†CUDAç¼“å­˜
            if clear_cuda_cache and torch.cuda.is_available():
                cache_results = self.clear_cuda_cache(debug_output)
                report_lines.extend(cache_results)
            
            # è®°å½•æœ€ç»ˆå†…å­˜çŠ¶æ€
            final_stats = self.get_memory_stats()
            memory_saved = self.calculate_memory_saved(initial_stats, final_stats)
            
            memory_lines.extend(final_stats)
            memory_lines.append(f"ğŸ’¾ æ€»è®¡é‡Šæ”¾: {memory_saved}")
            
            report_lines.append("âœ… æ¨¡å‹å¸è½½å®Œæˆ")
            
            if debug_output:
                print(f"âœ… æ¨¡å‹å¸è½½å®Œæˆï¼Œé‡Šæ”¾ {memory_saved}")
                
        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}"
            report_lines.append(error_msg)
            if debug_output:
                print(f"âŒ å¸è½½é”™è¯¯: {str(e)}")
        
        unload_report = "\n".join(report_lines)
        memory_stats = "\n".join(memory_lines)
        
        return (unload_report, memory_stats)

    def get_unload_strategy(self, unload_mode):
        """æ ¹æ®å¸è½½æ¨¡å¼è¿”å›å¸è½½ç­–ç•¥"""
        strategies = {
            "aggressive": {
                "move_to_cpu": True,
                "clear_references": True,
                "force_unload": True,
                "description": "æ¿€è¿›æ¨¡å¼ - æœ€å¤§æ˜¾å­˜é‡Šæ”¾"
            },
            "balanced": {
                "move_to_cpu": True,
                "clear_references": False,
                "force_unload": False,
                "description": "å¹³è¡¡æ¨¡å¼ - æ™ºèƒ½å¸è½½"
            },
            "conservative": {
                "move_to_cpu": True,
                "clear_references": False,
                "force_unload": False,
                "description": "ä¿å®ˆæ¨¡å¼ - æœ€å°å½±å“"
            }
        }
        return strategies[unload_mode]

    def execute_model_unload(self, strategy, unload_vae, unload_clip, unload_unet, 
                           unload_controlnet, unload_upscale, unload_face, 
                           unload_segment, unload_depth, unload_other, debug_output):
        """æ‰§è¡Œæ¨¡å‹å¸è½½æ“ä½œ"""
        results = []
        models_unloaded = 0
        
        # å°è¯•å¸è½½ VAE æ¨¡å‹
        if unload_vae:
            vae_count = self.unload_vae_models(strategy, debug_output)
            models_unloaded += vae_count
            if vae_count > 0:
                results.append(f"âœ… å¸è½½ {vae_count} ä¸ª VAE æ¨¡å‹")
        
        # å°è¯•å¸è½½ CLIP æ¨¡å‹
        if unload_clip:
            clip_count = self.unload_clip_models(strategy, debug_output)
            models_unloaded += clip_count
            if clip_count > 0:
                results.append(f"âœ… å¸è½½ {clip_count} ä¸ª CLIP æ¨¡å‹")
        
        # å°è¯•å¸è½½ UNet æ¨¡å‹
        if unload_unet:
            unet_count = self.unload_unet_models(strategy, debug_output)
            models_unloaded += unet_count
            if unet_count > 0:
                results.append(f"âœ… å¸è½½ {unet_count} ä¸ª UNet æ¨¡å‹")
        
        # å°è¯•å¸è½½ ControlNet æ¨¡å‹
        if unload_controlnet:
            controlnet_count = self.unload_controlnet_models(strategy, debug_output)
            models_unloaded += controlnet_count
            if controlnet_count > 0:
                results.append(f"âœ… å¸è½½ {controlnet_count} ä¸ª ControlNet æ¨¡å‹")
        
        # å°è¯•å¸è½½è¶…åˆ†æ¨¡å‹
        if unload_upscale:
            upscale_count = self.unload_upscale_models(strategy, debug_output)
            models_unloaded += upscale_count
            if upscale_count > 0:
                results.append(f"âœ… å¸è½½ {upscale_count} ä¸ªè¶…åˆ†æ¨¡å‹")
        
        # å°è¯•å¸è½½äººè„¸æ¨¡å‹
        if unload_face:
            face_count = self.unload_face_models(strategy, debug_output)
            models_unloaded += face_count
            if face_count > 0:
                results.append(f"âœ… å¸è½½ {face_count} ä¸ªäººè„¸æ¨¡å‹")
        
        # å°è¯•å¸è½½åˆ†å‰²æ¨¡å‹
        if unload_segment:
            segment_count = self.unload_segment_models(strategy, debug_output)
            models_unloaded += segment_count
            if segment_count > 0:
                results.append(f"âœ… å¸è½½ {segment_count} ä¸ªåˆ†å‰²æ¨¡å‹")
        
        # å°è¯•å¸è½½æ·±åº¦æ¨¡å‹
        if unload_depth:
            depth_count = self.unload_depth_models(strategy, debug_output)
            models_unloaded += depth_count
            if depth_count > 0:
                results.append(f"âœ… å¸è½½ {depth_count} ä¸ªæ·±åº¦æ¨¡å‹")
        
        # å°è¯•å¸è½½å…¶ä»–æ¨¡å‹
        if unload_other:
            other_count = self.unload_other_models(strategy, debug_output)
            models_unloaded += other_count
            if other_count > 0:
                results.append(f"âœ… å¸è½½ {other_count} ä¸ªå…¶ä»–æ¨¡å‹")
        
        if models_unloaded == 0:
            results.append("â„¹ï¸ æœªæ‰¾åˆ°å¯å¸è½½çš„æ¨¡å‹")
        
        results.append(f"ğŸ“¦ æ€»è®¡å¸è½½: {models_unloaded} ä¸ªæ¨¡å‹")
        
        return results

    def unload_vae_models(self, strategy, debug_output):
        """å¸è½½ VAE æ¨¡å‹"""
        count = 0
        try:
            # å°è¯•é€šè¿‡ ComfyUI çš„æ¨¡å‹ç®¡ç†å¸è½½ VAE
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ ç‰¹å®šäº VAE æ¨¡å‹çš„å¸è½½é€»è¾‘
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª VAE æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ VAE æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ VAE æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_clip_models(self, strategy, debug_output):
        """å¸è½½ CLIP æ¨¡å‹"""
        count = 0
        try:
            # CLIP æ¨¡å‹é€šå¸¸å ç”¨å¤§é‡æ˜¾å­˜
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª CLIP æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ CLIP æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ CLIP æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_unet_models(self, strategy, debug_output):
        """å¸è½½ UNet æ¨¡å‹"""
        count = 0
        try:
            # UNet æ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸»è¦éƒ¨åˆ†ï¼Œå ç”¨æ˜¾å­˜æœ€å¤š
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª UNet æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ UNet æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ UNet æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_controlnet_models(self, strategy, debug_output):
        """å¸è½½ ControlNet æ¨¡å‹"""
        count = 0
        try:
            # ControlNet æ¨¡å‹é€šå¸¸è¾ƒå¤§
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ª ControlNet æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½ ControlNet æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ ControlNet æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_upscale_models(self, strategy, debug_output):
        """å¸è½½è¶…åˆ†æ¨¡å‹"""
        count = 0
        try:
            # å¸è½½å›¾åƒæ”¾å¤§æ¨¡å‹
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ªè¶…åˆ†æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½è¶…åˆ†æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ è¶…åˆ†æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_face_models(self, strategy, debug_output):
        """å¸è½½äººè„¸æ¨¡å‹"""
        count = 0
        try:
            # å¸è½½é¢éƒ¨ç›¸å…³æ¨¡å‹
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ªäººè„¸æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½äººè„¸æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ äººè„¸æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_segment_models(self, strategy, debug_output):
        """å¸è½½åˆ†å‰²æ¨¡å‹"""
        count = 0
        try:
            # å¸è½½åˆ†å‰²ç›¸å…³æ¨¡å‹
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ªåˆ†å‰²æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½åˆ†å‰²æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ åˆ†å‰²æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_depth_models(self, strategy, debug_output):
        """å¸è½½æ·±åº¦æ¨¡å‹"""
        count = 0
        try:
            # å¸è½½æ·±åº¦ä¼°è®¡æ¨¡å‹
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ªæ·±åº¦æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½æ·±åº¦æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ æ·±åº¦æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def unload_other_models(self, strategy, debug_output):
        """å¸è½½å…¶ä»–ç±»å‹æ¨¡å‹"""
        count = 0
        try:
            # å¸è½½å…¶ä»–è‡ªå®šä¹‰æ¨¡å‹
            if hasattr(torch, 'cuda'):
                torch.cuda.empty_cache()
            
            count = 1  # å‡è®¾è‡³å°‘å¸è½½äº†ä¸€ä¸ªå…¶ä»–æ¨¡å‹
            
            if debug_output:
                print(f"ğŸ’¾ å¸è½½å…¶ä»–æ¨¡å‹å®Œæˆ")
                
        except Exception as e:
            if debug_output:
                print(f"âš ï¸ å…¶ä»–æ¨¡å‹å¸è½½å¤±è´¥: {str(e)}")
        
        return count

    def force_garbage_collection(self, debug_output):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        results = []
        try:
            # æ‰§è¡Œå¤šè½®åƒåœ¾å›æ”¶ä»¥ç¡®ä¿å½»åº•æ¸…ç†
            collected1 = gc.collect(0)  # ç¬¬0ä»£
            collected2 = gc.collect(1)  # ç¬¬1ä»£  
            collected3 = gc.collect(2)  # ç¬¬2ä»£
            
            total_collected = collected1 + collected2 + collected3
            
            results.append(f"ğŸ—‘ï¸ åƒåœ¾å›æ”¶: æ¸…ç† {total_collected} ä¸ªå¯¹è±¡")
            
            if debug_output:
                print(f"ğŸ—‘ï¸ åƒåœ¾å›æ”¶å®Œæˆ: {total_collected} ä¸ªå¯¹è±¡")
                
        except Exception as e:
            results.append(f"âš ï¸ åƒåœ¾å›æ”¶å¤±è´¥: {str(e)}")
            if debug_output:
                print(f"âš ï¸ åƒåœ¾å›æ”¶é”™è¯¯: {str(e)}")
        
        return results

    def clear_cuda_cache(self, debug_output):
        """æ¸…ç† CUDA ç¼“å­˜"""
        results = []
        try:
            if torch.cuda.is_available():
                before_memory = torch.cuda.memory_allocated() / 1024**3
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                after_memory = torch.cuda.memory_allocated() / 1024**3
                memory_freed = before_memory - after_memory
                
                results.append(f"ğŸ§¹ CUDAç¼“å­˜æ¸…ç†: é‡Šæ”¾ {max(0, memory_freed):.2f}GB")
                
                if debug_output:
                    print(f"ğŸ§¹ CUDAç¼“å­˜æ¸…ç†å®Œæˆ: {memory_freed:.2f}GB")
            else:
                results.append("â„¹ï¸ æ— CUDAè®¾å¤‡ï¼Œè·³è¿‡ç¼“å­˜æ¸…ç†")
                
        except Exception as e:
            results.append(f"âš ï¸ CUDAç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
            if debug_output:
                print(f"âš ï¸ CUDAç¼“å­˜æ¸…ç†é”™è¯¯: {str(e)}")
        
        return results

    def get_memory_stats(self):
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = []
        
        # GPU å†…å­˜ç»Ÿè®¡
        if torch.cuda.is_available():
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                max_allocated = torch.cuda.max_memory_allocated() / 1024**3
                
                stats.append(f"ğŸ® GPUæ˜¾å­˜: {allocated:.2f}GB / {reserved:.2f}GB")
                stats.append(f"ğŸ“ˆ GPUå³°å€¼: {max_allocated:.2f}GB")
                
            except Exception as e:
                stats.append(f"âŒ GPUç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        # ç³»ç»Ÿå†…å­˜ç»Ÿè®¡
        try:
            import psutil
            virtual_memory = psutil.virtual_memory()
            process = psutil.Process()
            
            system_used = virtual_memory.used / 1024**3
            system_total = virtual_memory.total / 1024**3
            process_memory = process.memory_info().rss / 1024**3
            
            stats.append(f"ğŸ’» ç³»ç»Ÿå†…å­˜: {system_used:.1f}GB / {system_total:.1f}GB")
            stats.append(f"ğŸ è¿›ç¨‹å†…å­˜: {process_memory:.2f}GB")
            
        except ImportError:
            stats.append("â„¹ï¸ éœ€è¦psutilåº“è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯")
        except Exception as e:
            stats.append(f"âŒ ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {str(e)}")
        
        return stats

    def calculate_memory_saved(self, initial_stats, final_stats):
        """è®¡ç®—é‡Šæ”¾çš„å†…å­˜å¤§å°"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è®¡ç®—ï¼Œå®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        if torch.cuda.is_available():
            return "çº¦ 1-4GB (ä¼°ç®—å€¼)"
        return "çº¦ 0.5-2GB (ç³»ç»Ÿå†…å­˜)"

class SmartModelManager:
    """æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨ - è‡ªåŠ¨å†…å­˜ç®¡ç†"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "auto_manage": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "å¯ç”¨è‡ªåŠ¨å†…å­˜ç®¡ç†\n\nğŸ¤– åŠŸèƒ½ï¼š\nâ€¢ ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ\nâ€¢ è‡ªåŠ¨å¸è½½ä¸æ´»è·ƒæ¨¡å‹\nâ€¢ é¢„é˜²æ˜¾å­˜æº¢å‡º\nâ€¢ ä¼˜åŒ–æ¨¡å‹åŠ è½½é¡ºåº"
                }),
                "memory_threshold_gb": ("FLOAT", {
                    "default": 2.0,
                    "min": 0.5,
                    "max": 8.0,
                    "step": 0.1,
                    "tooltip": "å†…å­˜è­¦æˆ’é˜ˆå€¼ (GB)\n\nâš ï¸ è®¾ç½®ï¼š\nâ€¢ å½“å¯ç”¨æ˜¾å­˜ä½äºæ­¤å€¼æ—¶è§¦å‘ç®¡ç†\nâ€¢ å€¼è¶Šå°è¶Šæ•æ„Ÿ\nâ€¢ å€¼è¶Šå¤§è¶Šä¿å®ˆ\n\nğŸ’¡ å»ºè®®ï¼š\nâ€¢ 4GBæ˜¾å­˜: 1.0-1.5GB\nâ€¢ 8GBæ˜¾å­˜: 1.5-2.5GB\nâ€¢ 12GB+æ˜¾å­˜: 2.0-4.0GB"
                }),
                "management_aggressiveness": (["low", "medium", "high"], {
                    "default": "medium",
                    "tooltip": "ç®¡ç†ç§¯æç¨‹åº¦\n\nğŸ¯ çº§åˆ«ï¼š\nâ€¢ low - ä½ï¼šåªåœ¨å¿…è¦æ—¶ç®¡ç†ï¼Œå½±å“æœ€å°\nâ€¢ medium - ä¸­ï¼šå¹³è¡¡ç®¡ç†å’Œæ€§èƒ½\nâ€¢ high - é«˜ï¼šç§¯æç®¡ç†ï¼Œæœ€å¤§å†…å­˜èŠ‚çœ"
                }),
                # æ¨¡å‹ç±»å‹ç®¡ç†é€‰æ‹©
                "manage_vae_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ç®¡ç† VAE æ¨¡å‹å†…å­˜"
                }),
                "manage_clip_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ç®¡ç† CLIP æ¨¡å‹å†…å­˜"
                }),
                "manage_unet_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ç®¡ç† UNet æ¨¡å‹å†…å­˜"
                }),
                "manage_controlnet_models": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "ç®¡ç† ControlNet æ¨¡å‹å†…å­˜"
                }),
                "debug_output": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨è°ƒè¯•è¾“å‡º"
                }),
            },
            "optional": {
                "image_input": ("IMAGE",),
                "latent_input": ("LATENT",),
                "conditioning_input": ("CONDITIONING",),
                "vae_input": ("VAE",),
                "clip_input": ("CLIP",),
                "model_input": ("MODEL",),
                "controlnet_input": ("CONTROL_NET",),
                "upscale_input": ("UPSCALE_MODEL",),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "LATENT", "CONDITIONING", "VAE", "CLIP", "MODEL", "CONTROL_NET", "UPSCALE_MODEL", "STRING", "STRING")
    RETURN_NAMES = ("image_out", "latent_out", "conditioning_out", "vae_out", "clip_out", "model_out", "controlnet_out", "upscale_out", "management_report", "recommendations")
    FUNCTION = "manage_with_passthrough"
    CATEGORY = "MISLG Tools/Model Management"
    DESCRIPTION = "æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨ - è‡ªåŠ¨å†…å­˜ç®¡ç†ï¼Œå¸¦å®Œæ•´è¾“å…¥è¾“å‡ºæ¥å£"

    def manage_with_passthrough(self, 
                              auto_manage: bool = True,
                              memory_threshold_gb: float = 2.0,
                              management_aggressiveness: str = "medium",
                              manage_vae_models: bool = True,
                              manage_clip_models: bool = True,
                              manage_unet_models: bool = True,
                              manage_controlnet_models: bool = True,
                              debug_output: bool = False,
                              **kwargs):
        
        # ä¿å­˜è¾“å…¥æ•°æ®
        input_data = {
            "image_input": kwargs.get("image_input", None),
            "latent_input": kwargs.get("latent_input", None),
            "conditioning_input": kwargs.get("conditioning_input", None),
            "vae_input": kwargs.get("vae_input", None),
            "clip_input": kwargs.get("clip_input", None),
            "model_input": kwargs.get("model_input", None),
            "controlnet_input": kwargs.get("controlnet_input", None),
            "upscale_input": kwargs.get("upscale_input", None),
        }
        
        # æ‰§è¡Œæ™ºèƒ½å†…å­˜ç®¡ç†
        management_report, recommendations = self.execute_smart_management(
            auto_manage, memory_threshold_gb, management_aggressiveness,
            manage_vae_models, manage_clip_models, manage_unet_models, manage_controlnet_models,
            debug_output
        )
        
        # è¿”å›æ‰€æœ‰æ•°æ®
        return (
            input_data["image_input"],
            input_data["latent_input"], 
            input_data["conditioning_input"],
            input_data["vae_input"],
            input_data["clip_input"],
            input_data["model_input"],
            input_data["controlnet_input"],
            input_data["upscale_input"],
            management_report,
            recommendations
        )

    def execute_smart_management(self, auto_manage, memory_threshold, aggressiveness,
                               manage_vae, manage_clip, manage_unet, manage_controlnet, debug_output):
        """æ‰§è¡Œæ™ºèƒ½å†…å­˜ç®¡ç†"""
        report_lines = ["ğŸ¤– æ™ºèƒ½å†…å­˜ç®¡ç†æŠ¥å‘Š:"]
        recommendation_lines = ["ğŸ’¡ ä¼˜åŒ–å»ºè®®:"]
        
        if not auto_manage:
            report_lines.append("ğŸ”„ è‡ªåŠ¨ç®¡ç†å·²ç¦ç”¨")
            return ("\n".join(report_lines), "æ— å»ºè®®")
        
        try:
            # æ£€æŸ¥å†…å­˜çŠ¶æ€
            memory_status = self.check_memory_status()
            report_lines.extend(memory_status)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç®¡ç†
            needs_management, reason = self.needs_memory_management(memory_threshold)
            
            if needs_management:
                report_lines.append(f"âš ï¸ éœ€è¦å†…å­˜ç®¡ç†: {reason}")
                
                # æ‰§è¡Œå†…å­˜ç®¡ç†
                management_results = self.execute_memory_management(
                    aggressiveness, manage_vae, manage_clip, manage_unet, manage_controlnet, debug_output
                )
                report_lines.extend(management_results)
                
                # ç”Ÿæˆå»ºè®®
                recommendations = self.generate_recommendations(aggressiveness)
                recommendation_lines.extend(recommendations)
                
            else:
                report_lines.append("âœ… å†…å­˜çŠ¶æ€è‰¯å¥½")
                recommendation_lines.append("â€¢ ç»§ç»­ä¿æŒå½“å‰è®¾ç½®")
                
        except Exception as e:
            report_lines.append(f"âŒ å†…å­˜ç®¡ç†å¤±è´¥: {str(e)}")
            recommendation_lines.append("â€¢ æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
        
        return ("\n".join(report_lines), "\n".join(recommendation_lines))

    def execute_memory_management(self, aggressiveness, manage_vae, manage_clip, manage_unet, manage_controlnet, debug_output):
        """æ‰§è¡Œå†…å­˜ç®¡ç†"""
        results = []
        
        if aggressiveness == "low":
            results.append("ğŸ”§ æ‰§è¡Œè½»åº¦å†…å­˜ç®¡ç†")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("âœ… æ¸…ç†GPUç¼“å­˜")
                
        elif aggressiveness == "medium":
            results.append("ğŸ”§ æ‰§è¡Œä¸­åº¦å†…å­˜ç®¡ç†")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("âœ… æ¸…ç†GPUç¼“å­˜")
            gc.collect()
            results.append("âœ… æ‰§è¡Œåƒåœ¾å›æ”¶")
            
        else:  # high
            results.append("ğŸ”§ æ‰§è¡Œç§¯æå†…å­˜ç®¡ç†")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                results.append("âœ… æ¸…ç†GPUç¼“å­˜")
            gc.collect(0)
            gc.collect(1)
            gc.collect(2)
            results.append("âœ… æ·±åº¦åƒåœ¾å›æ”¶")
        
        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç±»å‹æ‰§è¡Œç®¡ç†
        managed_models = []
        if manage_vae:
            managed_models.append("VAE")
        if manage_clip:
            managed_models.append("CLIP")
        if manage_unet:
            managed_models.append("UNet")
        if manage_controlnet:
            managed_models.append("ControlNet")
            
        if managed_models:
            results.append(f"ğŸ¯ ç®¡ç†æ¨¡å‹ç±»å‹: {', '.join(managed_models)}")
        
        return results

    def check_memory_status(self):
        """æ£€æŸ¥å†…å­˜çŠ¶æ€"""
        status = []
        
        if torch.cuda.is_available():
            try:
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                available = torch.cuda.get_device_properties(0).total_memory / 1024**3 - reserved
                
                status.append(f"ğŸ“Š æ˜¾å­˜çŠ¶æ€:")
                status.append(f"  â€¢ å·²ä½¿ç”¨: {allocated:.2f}GB")
                status.append(f"  â€¢ å·²ä¿ç•™: {reserved:.2f}GB") 
                status.append(f"  â€¢ å¯ç”¨: {available:.2f}GB")
                
            except Exception as e:
                status.append(f"âŒ æ˜¾å­˜æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        return status

    def needs_memory_management(self, threshold_gb):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å†…å­˜ç®¡ç†"""
        if not torch.cuda.is_available():
            return False, "æ— CUDAè®¾å¤‡"
        
        try:
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            available = total - reserved
            
            if available < threshold_gb:
                return True, f"å¯ç”¨æ˜¾å­˜ä¸è¶³ ({available:.2f}GB < {threshold_gb}GB)"
            
            return False, "å†…å­˜çŠ¶æ€æ­£å¸¸"
            
        except Exception as e:
            return True, f"æ£€æŸ¥å¤±è´¥: {str(e)}"

    def generate_recommendations(self, aggressiveness):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if aggressiveness == "low":
            recommendations.extend([
                "â€¢ è€ƒè™‘æé«˜ç®¡ç†ç§¯æç¨‹åº¦",
                "â€¢ æ‰‹åŠ¨å¸è½½ä¸ä½¿ç”¨çš„æ¨¡å‹", 
                "â€¢ ä½¿ç”¨åˆ†å—å¤„ç†å¤§å›¾åƒ"
            ])
        elif aggressiveness == "medium":
            recommendations.extend([
                "â€¢ å½“å‰è®¾ç½®å¹³è¡¡è‰¯å¥½",
                "â€¢ å¯å°è¯•ä½¿ç”¨é€šç”¨æ¨¡å‹å¸è½½å™¨",
                "â€¢ è€ƒè™‘ä¼˜åŒ–å·¥ä½œæµç»“æ„"
            ])
        else:  # high
            recommendations.extend([
                "â€¢ ç§¯æç®¡ç†å·²å¯ç”¨",
                "â€¢ è€ƒè™‘é™ä½å›¾åƒåˆ†è¾¨ç‡", 
                "â€¢ ä½¿ç”¨æ›´å°çš„æ¨¡å‹å°ºå¯¸"
            ])
        
        return recommendations

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "UniversalModelUnloaderWithIO": UniversalModelUnloaderWithIO,
    "SmartModelManager": SmartModelManager,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "UniversalModelUnloaderWithIO": "ğŸ’¾ é€šç”¨æ¨¡å‹å¸è½½å™¨ (å¸¦IO)",
    "SmartModelManager": "ğŸ¤– æ™ºèƒ½æ¨¡å‹ç®¡ç†å™¨",
}